---
title: "Forecasting tornado count distributions by EF damage rating"
author: "James Elsner"
date: "2/14/2019"
output: github_notebook
editor_options:
  chunk_output_type: console
---
Abstract for European SLS conference

Abstract Deadline: April 24, 2019
Submission link: https://meetings.copernicus.org/ecss2019/abstract_management/

Predicting tornado count distributions by EF rating

Elsner/Schroder

The science of forecasting where and when an outbreak of severe convective weather might occur is well developed. But more work is needed to anticipate specific characteristics of the outbreaks. Here we develop a statistical model that can be used to predict the probability distribution of tornado counts by EF rating on days expected to produce an outbreak of tornadoes in the United States. The model is fit using demographic data to condition on the fact that damage ratings are related to the number (and type) of potential targets. The model quantifies the importance of this demographic data relative to data describing environmental factors known to influence tornadoes, including CAPE and bulk shear. As a test case, the model controls for distance to nearest city while quantifying the percent increase in the chance of at least one tornado rated EF3 or higher for a unit increase in CAPE and bulk shear. The model is an ordered logistic regression and the coefficients are determined using the method of Hamiltonian Monte Carlo with the Stan language. Stan code is generated from R through the 'rethinking' package. The flexibility of this approach allows easy modifications based on domain-specific knowledge.

Ordered logistic regression to predict the probability distributions of tornado counts by EF rating. With distance to nearest city as a predictor we will be able to see how population shifts the distributions for example from EF0 to EF1 from EF1 to EF2. We know that the chance of getting an EF5 tornado in the data set increases with the number of structures. What we don't know is by how much relative to an EF3 or EF4.

Load packages.
```{r}
library(sf)
library(tmap)
library(dplyr)
library(ggplot2)
```

Load the data. This study uses the outbreak data collated in the tor-cluster GitHub project.
```{r}
load("~/Desktop/Projects/EF-dist/BigDays.RData")
st_crs(BigDayTornadoes)
st_crs(BigDays.sfdfT)
```

The simple feature `BigDays.sfdfT` contains outbreak-level data including environmental variables. The simple feature `BigDayTornadoes` contains tornado-level data. Note: they have different CRSs.

How many convective days have 10 or more tornadoes?
```{r}
All_Tornadoes %>%
  group_by(cDate) %>%
  summarize(nT = n()) %>%
  filter(nT >= 10) %>%
  dim()
```

Of these 870 big convective days we only consider 212 that occurred as part of a big outbreak.

A map of a big tornado day.
```{r}
library(USAboundaries)
ctys <- us_counties()
sts1 <- us_states(states = c("IL", "IN", "KY", "MO", "KS",
                            "OK", "AR", "TN", "MS", "TX"))
sts <- us_states()

groupNo <- 53

BD <- BigDays.sfdfT %>%
  filter(groupNumber == groupNo)
BDt <- BigDayTornadoes %>%
  filter(groupNumber == groupNo) %>%
  mutate(magF = factor(mag, levels = 0:4))

library(tmap)

tm_shape(BD) +
  tm_polygons() +
tm_shape(sts1, is.master = TRUE) +
  tm_borders() +
tm_shape(sts) +
  tm_borders(col = "gray70") +
  tm_text("name", size = .6) +
tm_shape(BDt) +
  tm_dots(size = .5) +
tm_compass(position = c("left", "bottom")) +
tm_scale_bar(position = c("left", "bottom")) +
tm_layout(title = "April 26, 1994")
```

Distribution of big convective days by month.
```{r}
ggplot(BigDays.sfdfT, aes(x = Month)) +
  geom_bar(fill = "gray70") +
  scale_x_discrete(labels = month.name) +
  scale_y_continuous(limits = c(0, 60)) +
  ylab("Number of Big Days") + xlab("") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(BigDays.sfdfT, aes(x = nT)) +
  geom_histogram(fill = "gray70", binwidth = 5) +
  scale_y_continuous(limits = c(0, NA)) +
  ylab("Number of Big Days") + xlab("Number of Tornadoes") +
  theme_minimal() 
```

We join the environmental variables at the outbreak-level to the data at the tornado level.
```{r}
df <- as.data.frame(BigDays.sfdfT) %>%
  select(ID, maxCAPE, maxHLCY, minCIN, maxBS, maxSM) %>%
  left_join(as.data.frame(BigDayTornadoes), by = "ID")
```

Compute the distance to nearest city/town.
```{r}
library(USAboundaries)

C.sf <- us_cities() %>%
  st_transform(crs = 102004)

dist <- numeric()
for(i in seq_len(nrow(BigDayTornadoes))) dist[i] <- min(st_distance(BigDayTornadoes[i, ], C.sf))

df$dist <- dist

#closest <- list()
#for(i in seq_len(nrow(BigDayTornadoes))){
#    closest[[i]] <- C.sf[which.min(st_distance(C.sf, BigDayTornadoes[i, ])), ]
#}

```

McElreath Lecture 14. Chapter 12 of his rethinking book.

Load the **rethinking** package.
```{r}
#devtools::install_github("rmcelreath/rethinking")
library(rethinking)
```


### Describing the ordered distribution of maximum EF ratings with intercepts

Histogram of maximum EF rating per tornado.
```{r}
df2 <- df %>%
  mutate(maxEF = as.integer(mag)) %>%
  select(maxEF, dist, maxEF, Year, maxCAPE, maxHLCY, maxBS, minCIN)

plot.df <- data.frame(table(df2$maxEF))
names(plot.df) <- c("EF", "Frequency")

ggplot(plot.df, aes(x = EF, y = Frequency)) + 
  geom_point() + 
  geom_segment(aes(xend = EF, yend = 0)) +
  xlab("Maximum EF Rating") +
  theme_minimal()
```

Our goal is to re-describe this histogram on the log-cumulative-odds scale. This means constructing the odds of a cumulative probability and then taking a logarithm. The logit is log-odds. The cumulative logit is log-cumulative-odds. Both constrain the probabilities to the 0/1 interval. When we add predictor variables, we can safely do so on the cumulative logit scale. The link function takes care of converting the parameter estimates to the proper probability scale.

We first compute the cumulative probabilities from the histogram. The discrete proportion of each EF ranking.
```{r}
pr_k <- as.vector(table(df2$maxEF) / nrow(df2))
cum_pr_k <- cumsum(pr_k)

plot.df <- data.frame(maxEF = 0:5, pr_k, cum_pr_k)

ggplot(plot.df, aes(x = maxEF, y = cum_pr_k)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1)) +
    xlab("Maximum EF Rating") +
    ylab("Cumulative Proportion") +
  theme_minimal()
```

Then to re-describe the histogram as log-cumulative odds, we need a series of intercept parameters. Each intercept will be on the log-cumulative-odds scale and stand in for the cumulative probability of each rating.
$$
\log \frac{\Pr(y_i \le k)}{1 - \Pr(y_i \le k)} = \alpha_k
$$
where $\alpha_k$ is an 'intercept' unique to each possible EF rating $k$. 

We compute these intercept parameters directly.
```{r}
( lco <- logit(cum_pr_k) )

plot.df$lco <- lco

ggplot(plot.df[1:5, ], aes(x = maxEF, y = lco)) +
  geom_point() +
  geom_line() +
  scale_y_continuous() +
  xlab("Maximum EF Rating") +
  ylab("log-cumulative-odds") +
  theme_minimal()
```

Note that the cumulative logit for the highest EF rating is infinity. This is because log(1/(1 - 1)) = $\infty$. This is always the case so we do not need a parameter for it. We get it for free from the law of total probability. So for $K$ = 6 possible maximum EF ratings we only need $K$ - 1 = 5 intercepts.

What we really want is the prior distribution of these intercepts. This allows us to take into account sample size and prior information, as well as insert predictor variables.

To use Bayes' theorem to compute the posterior distribution of these intercepts, we will need to compute the likelihood for each possible EF rating. So the last step in constructing the basic model fitting engine for these ordered categorical outcomes is to use cumulative probabilities $\Pr(y_i \le k)$ to compute the likelihood $\Pr(y_i = k)$.

```{r}
plot.df$ys <-  plot.df$cum_pr_k - plot.df$pr_k

ggplot(plot.df, aes(x = maxEF, y = cum_pr_k)) +
  geom_point() +
  geom_line() +
  geom_segment(aes(x = maxEF, xend = maxEF, y = ys, yend = cum_pr_k), size = 2, color = "blue") +
  scale_y_continuous(limits = c(0, 1)) +
    xlab("Maximum EF Rating") +
    ylab("Cumulative Proportion") +
  theme_minimal()
```

Cumulative probability and ordered likelihood. The horizontal axis displays possible observable damage ratings, from 0 through 5. The vertical axis displays cumulative probability. The points show cumulative probability. These keep getting higher with each successive EF rating. The blue line segments show the discrete probability of each EF rating. These are the likelihoods that go into Bayesâ€™ theorem.

In code form
$$
\begin{aligned} 
\hbox{R}_i &\sim \hbox{Ordered}(\mathbf{p}) \\ 
\hbox{logit}(p_k) &= \alpha_k \\
\alpha_k &\sim \hbox{Normal}(0, 10)
\end{aligned}
$$

The Ordered distribution is a categorical distribution that takes a vector $p = \{p_0, p_1, p_2, p_3, p_4\}$ of probabilities for each EF rating below the highest (EF5). Each response value $k$ in this vector is defined by its link to an intercept parameter ($\alpha_k$). 

Finally, some weakly regularizing priors are placed on these intercepts. In this case, there is a lot of data so just about any prior will be overwhelmed.

In code form. Note we can't use zero as a category so we add 1 to the variable `maxEF`.
```{r}
df2$maxEF1 <- df2$maxEF + 1
m1.1 <- map(
  alist(
    maxEF1 ~ dordlogit( phi, c(a0, a1, a2, a3, a4) ),
    phi <- 0,
    c(a0, a1, a2, a3, a4) <- dnorm(0, 10)
  ) ,
  data = df2 ,
  start = list(a0 = -1, a1 = 0, a2 = 1, a3 = 2, a4 = 3)
)
```

The model returns the same values as those in `cum_pr_k`.
```{r}
logistic(coef(m1.1))
```

Using Stan's HMC engine.
```{r}
m1.1stan <- map2stan(
    alist(
        maxEF1 ~ dordlogit( phi , cutpoints ),
        phi <- 0,
        cutpoints ~ dnorm(0, 10)
),
data = list(maxEF1 = df2$maxEF1), 
start = list(cutpoints = c(-2, -1, 0, 2, 3)), 
chains = 2 , cores = 2 )
```

```{r}
# need depth=2 to show vector of parameters
precis(m1.1stan, depth = 2)
```

The individual `cutpoints` parameters correspond to each $\alpha_k$ from earlier.

### Adding a predictor variable

To include predictor variables, we define the log-cumulative-odds of each EF rating $k$ as a sum of its intercept $\alpha_k$ and a typical linear model. Suppose for  example we want to add a predictor $x$ to the model. We do this by defining a linear model $\phi_i = \beta x_i$. Then each cumulative logit becomes
$$
\begin{aligned}
\log \frac{\Pr(y_i \le k)}{1 - \Pr(y_i \le k)} &= \alpha_k - \phi_i \\
\phi_i &= \beta x_i
\end{aligned}
$$

The form automatically ensures the correct ordering of the EF ratings, while still morphing the likelihood of each individual value as the predictor $x_i$ changes value. Why is the linear model $\phi$ subtracted from each intercept? Because if we decrease the log-cumulative odds of every outcome value $k$ below the maximum, this necessarily shifts the probability mass upwards towards higher outcome values. Recall quantile regression.

For example, suppose we take the MAP estimates from `m1.1` and subtract .5 from each. The function `dordlogit()` makes the calculation of likelihoods straitforward.
```{r}
( pk <- dordlogit(1:5, 0, coef(m1.1)) )
```

These probabilities imply an average outcome value of
```{r}
sum(pk * (1:5))
```

And now subtracting .5 from each
```{r}
pk <- dordlogit(1:5, 0, coef(m1.1) - .5)
sum(pk * (1:5))
```

The expected value has increased to 2.0.

And that is why we subtract $\phi$, the linear model $\beta x_i$ from each intercept. This way a positive $\beta$ value indicates that an increase in the predictor variable $x$ results in an increase in the average response.

Create scaled predictor variables.
```{r}
df2$Yrs <- scale(df2$Year)
df2$dists <- scale(df2$dist)
df2$CAPEs <- scale(df2$maxCAPE)
df2$HLCYs <- scale(df2$maxHLCY)
df2$BSs <- scale(df2$maxBS)
df2$CINs <- scale(df2$minCIN)
```

We fit this model by adding the slope parameters and predictor variables to the `phi` parameter inside `dordlogit()` function.
```{r}
m1.2 <- map(
  alist(
    maxEF1 ~ dordlogit( phi, c(a0, a1, a2, a3, a4) ),
    phi <- bYr * Yrs + bDis * dists + bCA * CAPEs + bH * HLCYs + bS * BSs + bCI * CINs,
    bYr ~ dnorm(0, 10),
    bDis ~ dnorm(0, 10),
    c(a0, a1, a2, a3, a4) <- dnorm(0, 10)
  ) ,
  data = df2 ,
  start = list(a0 = -1, a1 = 0, a2 = 1, a3 = 2, a4 = 3, 
               bYr = 0, bDis = 0, bCA = 0, bH = 0, bS = 0, bCI = 0)
)
```

### Plot the posterior predictions

Extract the posterior values of the parameters.
```{r}
post.df <- extract.samples(m1.2)
```


#### Bulk shear

Consider how the model handles bulk shear. Set all variables to zero except distance to city and bulk shear. Set the distance to city to a small value (near the city). The scale and center parameters of the bulk shear variable are obtained and then extremes values are used.
```{r}
BSs <- scale(df2$maxBS)
scale <- attr(BSs, "scaled:scale")
center <- attr(BSs, "scaled:center")
( BSlo <- (15 - center) / scale )
( BShi <- (45 - center) / scale )
```

Line plots.
```{r}
dists <- -1.3
Yrs <- 0
CAPEs <- 0
HLCYs <- 0
BSs <- c(BSlo, BShi)
CINs <- 0

tpk <- numeric()
for ( s in 1:10 ) {
    p <- post.df[s, ]
    ak <- as.numeric(p[1:5])
    phi <- p$bS * BSs
    pk <- pordlogit( 1:5 , a = ak , phi = phi )
    tpk <- rbind(tpk, pk)
}

tpk.df <- as.data.frame(tpk)
tpk.df$X <- rep(c(15, 45), times = 10)
tpk.df$S <- rep(1:10, each = 2)
names(tpk.df) <- c("EF0", "EF1", "EF2", "EF3", "EF4", "X", "S")

ggplot(tpk.df, aes(x = X, y = EF0, group = S)) +
  geom_line(color = "#EFF3FF") +
  geom_line(aes(y = EF1), color = "#BDD7E7") +
  geom_line(aes(y = EF2), color = "#6BAED6") +
  geom_line(aes(y = EF3), color = "#3182BD") +
  geom_line(aes(y = EF4), color = "#08519C") +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous() +
  xlab("Bulk Shear (m/s)") + ylab("Cumulative Proportion") +
  theme_minimal()
```

Area plots.
```{r}
#p <- post.df[1, ]
p <- colMeans(post.df)

ak <- as.numeric(p[1:5])
phi <- p[10] * BSs[1]
dk1 <- dordlogit( 1:5 , a = ak , phi = phi )

phi <- p[10] * BSs[2]
dk2 <- dordlogit( 1:5 , a = ak , phi = phi )

dk <- c(dk1, dk2)

tpk.df <- data.frame(dk,
                     BS = rep(c(15, 45), each = 5),
                     EF = factor(rep(c("0", "1", "2", "3", "4"), times = 2), 
                                 levels = c("4", "3", "2", "1", "0")))

ggplot(tpk.df, aes(x = BS, y = dk)) + 
  geom_area(aes(fill = EF), color = "white") +
  scale_fill_brewer(type = "seq", direction = -1) +
  xlab("Bulk Shear (m/s)") + ylab("Cumulative Proportion") +
  theme_minimal() +
  ggtitle(label = "Predicted proportion of tornadoes by EF category") 

```

#### Distance to city

Consider how the model handles distance from city. Set all variables to zero except distance from city and bulk shear. Set bulk shear to a high value. The scale and center parameters of the bulk shear variable are obtained and then extremes values are used.
```{r}
dists <- scale(df2$dist)
scale <- attr(dists, "scaled:scale")
center <- attr(dists, "scaled:center")

( Dclose <- (0 - center) / scale )
( Dfar <- (100000 - center) / scale )
```

Line plots.
```{r}
dists <- c(Dclose, Dfar)
Yrs <- 0
CAPEs <- 0
HLCYs <- 0
BSs <- 2.2
CINs <- 0

tpk <- numeric()
for ( s in 1:10 ) {
    p <- post.df[s, ]
    ak <- as.numeric(p[1:5])
    phi <- p$bS * BSs + p$bDis * dists
    pk <- pordlogit( 1:5 , a = ak , phi = phi )
    tpk <- rbind(tpk, pk)
}

tpk.df <- as.data.frame(tpk)
tpk.df$X <- rep(c(0, 100000), times = 10)
tpk.df$S <- rep(1:10, each = 2)
names(tpk.df) <- c("EF0", "EF1", "EF2", "EF3", "EF4", "X", "S")

ggplot(tpk.df, aes(x = X, y = EF0, group = S)) +
  geom_line(color = "#EFF3FF") +
  geom_line(aes(y = EF1), color = "#BDD7E7") +
  geom_line(aes(y = EF2), color = "#6BAED6") +
  geom_line(aes(y = EF3), color = "#3182BD") +
  geom_line(aes(y = EF4), color = "#08519C") +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous() +
  xlab("Distance from city (m)") + ylab("Cumulative Proportion") +
  theme_minimal()
```

Area plots.
```{r}
#p <- post.df[1, ]
p <- colMeans(post.df)
ak <- as.numeric(p[1:5])
phi <-  p[10] * BSs + p[7] * dists[1]
dk1 <- dordlogit( 1:5 , a = ak , phi = phi )

phi <- p[10] * BSs + p[7] * dists[2]
dk2 <- dordlogit( 1:5 , a = ak , phi = phi )

dk <- c(dk1, dk2)

tpk.df <- data.frame(dk,
                     Dc = rep(c(0, 100000), each = 5),
                     EF = factor(rep(c("0", "1", "2", "3", "4"), times = 2), 
                                 levels = c("4", "3", "2", "1", "0")))

ggplot(tpk.df, aes(x = Dc/1000, y = dk)) + 
  geom_area(aes(fill = EF), color = "white") +
  scale_fill_brewer(type = "seq", direction = -1) +
  xlab("Distance from nearest city/town (km)") + ylab("Cumulative Proportion") +
  theme_minimal() +
  ggtitle(label = "Predicted proportion of tornadoes by EF category") 

```