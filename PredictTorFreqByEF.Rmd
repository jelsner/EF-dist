---
title: "Forecasting tornado count distributions by EF damage rating"
author: "James Elsner"
date: "2/14/2019"
output: github_notebook
editor_options:
  chunk_output_type: console
---
Abstract for the European SLS conference in Krakow, POLAND, 4-8 November 2019.

Abstract Deadline: April 24, 2019
Submission link: https://meetings.copernicus.org/ecss2019/abstract_management/

Predicting tornado count distributions by EF rating

Elsner/Schroder

_Journal of Applied Meteorology and Climatology_

The science of forecasting where an outbreak of severe convective weather might occur days in advance is well developed. For example, considerable skill exists in outlining what regions of the United States have the highest risk of severe weather on any given day. But more work is needed to anticipate specific characteristics of the outbreaks. Here we develop a statistical model that can be used to predict the probability distribution of tornado counts by EF rating on days expected to produce an outbreak. The model is fit using population centers to condition on the fact that damage ratings are related to the number (and type) of potential targets. The model quantifies the importance of this demographic information relative to information from environmental factors known to influence tornadoes, including CAPE and bulk shear. The model controls for distance to nearest city to quantify the percent increase in the chance of at least one tornado rated EF3 or higher for a unit increase in CAPE and bulk shear. The model is an ordered logistic regression and the coefficients are determined using the method of Hamiltonian Monte Carlo with the Stan language. Stan code is generated from R through the 'brms' package. The flexibility of this approach makes it easy to adjust the model to forecast other outbreak characteristics and to include domain-specific knowledge.

Motivation: Current lack of statistical guidance for forecasting outbreak characteristics. Is this true? Perhaps such tools can be used to better describe the threat and thus hopefully save lives.

Ordered logistic regression to predict the probability distributions of tornado counts by EF rating. With distance to nearest city as a predictor we will be able to see how population shifts the distributions for example from EF0 to EF1 from EF1 to EF2. We know that the chance of getting an EF4+ tornado in the data set increases with the number of structures. What we don't know is by how much relative to an EF3.

The study aims to show: (1) the feasibility of forecasting a particular outbreak characteristic, and (2) the flexibility of a modeling framework that can be applied in forecasting other characteristics.

Load packages.
```{r}
library(sf)
library(tmap)
library(dplyr)
library(ggplot2)
```

Load the data. The study uses the data collated in the `tor-cluster` GitHub project. The methodology for determining the big cluster days is detailed in Schroder and Elsner (2019). Briefly, the method groups tornadoes occurring in the period 1994 through 2017 based on space-time distances between genesis locations. If two tornadoes occur close together in space and time, they are grouped together in a cluster. Grouping stops when the the space-time distance threshold exceeds 50K meter-seconds.
```{r}
load("~/Desktop/Projects/EF-dist/BigDays.RData")
st_crs(Torn.sfT)
st_crs(BigDays.sfdfT)
```

The simple feature `BigDays.sfdfT` contains cluster-level data including environmental variables. The simple feature `Torn.sfT` contains tornado-level data for all tornadoes on the big days. Note: they have different CRSs.

A map of one tornado group.
```{r}
library(USAboundaries)
ctys <- us_counties()
sts1 <- us_states(states = c("IL", "IN", "KY", "MO", "KS",
                            "OK", "AR", "TN", "MS", "TX"))
sts <- us_states()

groupNo <- 53

BD <- BigDays.sfdfT %>%
  filter(groupNumber == groupNo)
BDt <- Torn.sfT %>%
  filter(groupNumber == groupNo) %>%
  mutate(magF = factor(mag, levels = 0:4))

library(tmap)

tm_shape(BD) +
  tm_polygons() +
tm_shape(sts1, is.master = TRUE) +
  tm_borders() +
tm_shape(sts) +
  tm_borders(col = "gray70") +
  tm_text("name", size = .6) +
tm_shape(BDt) +
  tm_dots(size = .5) +
tm_compass(position = c("left", "bottom")) +
tm_scale_bar(position = c("left", "bottom")) +
tm_layout(title = "April 26, 1994")
```

Distribution of clusters by month.
```{r}
ggplot(BigDays.sfdfT, aes(x = as.factor(Month))) +
  geom_bar(fill = "gray70") +
  scale_x_discrete(labels = month.name) +
  scale_y_continuous(limits = c(0, NA)) +
  ylab("Number of Clusters\nWith At Least 10 Tornadoes") + xlab("") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(BigDays.sfdfT, aes(x = Month)) +
  geom_bar(fill = "gray70") +
  scale_x_continuous(breaks = seq(1, 12, 1), labels = month.abb) +
  scale_y_continuous(limits = c(0, NA)) +
  coord_polar(start = 0) +
  labs(x = "Month", y = "Number of Clusters\nWith At Least 10 Tornadoes") +
  theme_minimal()

ggplot(BigDays.sfdfT, aes(x = nT)) +
  geom_histogram(fill = "gray70", binwidth = 5) +
  scale_y_continuous(limits = c(0, NA)) +
  ylab("Number of Clusters") + xlab("Number of Tornadoes") +
  theme_minimal() 
```

We join the environmental variables at the cluster-level with the data at the tornado level.
```{r}
df <- as.data.frame(BigDays.sfdfT) %>%
  select(ID, maxCAPE, maxHLCY, minCIN, maxBS, maxSM) %>%
  left_join(as.data.frame(Torn.sfT), by = "ID")
```

Compute the distance to nearest city/town.
```{r}
library(USAboundaries)

C.sf <- us_cities() %>%
  st_transform(crs = 102004)

dist <- numeric()
for(i in seq_len(nrow(Torn.sfT))){
  dist[i] <- min(st_distance(Torn.sfT[i, ], C.sf))
}

df$dist <- dist
```

Returns the closest cities as a data frame.
```{r}
closest.df <- numeric()
 for(i in seq_len(nrow(Torn.sfT))){
     closest.df <- rbind(closest.df, as.data.frame(C.sf[which.min(st_distance(C.sf, Torn.sfT[i, ])), ]))
 }

closest.df %>%
  group_by(stplfips_2010) %>%
  summarize(nT = n()) %>%
  arrange(desc(nT))

closest.df %>%
  filter(stplfips_2010 == 2018250)
closest.df %>%
  filter(stplfips_2010 == 2063600)
```

Dodge City KS (2010 population of 27,340) and Scott City KS (3,816) are the two cities with the most 'nearby' tornadoes.

### Rethinking package

McElreath Lecture 14. Chapter 12 of his rethinking book.

Load the **rethinking** package.
```{r}
#devtools::install_github("rmcelreath/rethinking")
library(rethinking)
```

### Describing the ordered distribution of maximum EF ratings with intercepts

We begin with a histogram of maximum EF rating per tornado.
```{r}
df2 <- df %>%
  mutate(maxEF = as.integer(mag)) %>%
  select(ID, maxEF, dist, maxEF, Year, mo, maxCAPE, maxHLCY, maxBS, minCIN)

plot.df <- data.frame(table(df2$maxEF))
names(plot.df) <- c("EF", "Frequency")

ggplot(plot.df, aes(x = EF, y = Frequency)) + 
  geom_point() + 
  geom_segment(aes(xend = EF, yend = 0)) +
  xlab("Maximum EF Rating") +
  theme_minimal()

table(df2$maxEF)/dim(df2)[1]
load("~/Desktop/Projects/EF-dist/BigDaysInLargeGroups.RData") # Note the file BigDays.sfdfT now is only days within a large group
table(All_Tornadoes$mag)/nrow(All_Tornadoes)
```
**Histogram of tornadoes by maximum EF rating. Only tornadoes occurring in large clusters are considered (see text).**

As expected the histogram by EF rating on large cluster days shows that the vast majority of tornadoes are rated EF0 or EF1 with far fewer rated EF4 or EF5. But relative to all tornadoes the distribution favors higher ratings. For example, 3.5\% of big outbreak day tornadoes are rated EF3 compared with 2.3\% of all tornadoes. And .08\% of big outbreak day tornadoes are rated EF5 compared with .05\% of all tornadoes. 

Next we describe this histogram on the log-cumulative-odds scale by constructing the odds of a cumulative probability and then taking logarithms. Since the logit is log-odds, the cumulative logit is log-cumulative-odds. Both the logit and cumulative logit constrain the probabilities to the interval between 0 and 1. When we add predictor variables, we do so on the cumulative logit scale. The link function takes care of converting the parameter estimates to the proper probability scale.

We first compute the cumulative probabilities from the histogram. The discrete proportion of each EF ranking.
```{r}
pr_k <- as.vector(table(df2$maxEF) / nrow(df2))
pr_k_all <- as.vector(table(All_Tornadoes$mag) / nrow(All_Tornadoes))
cum_pr_k <- cumsum(pr_k)
cum_pr_k_all <- cumsum(pr_k_all)

plot.df <- data.frame(maxEF = 0:5, pr_k, cum_pr_k, cum_pr_k_all)

(p1 <- ggplot(plot.df, aes(x = maxEF, y = cum_pr_k_all)) +
  geom_point(color = "gray70") +
  geom_line(color = "gray70") +
  geom_point(aes(y = cum_pr_k)) +
  geom_line(aes(y = cum_pr_k)) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous(minor_breaks = 0:5) +
    xlab("Maximum EF Rating") +
    ylab("Cumulative Proportion") +
    ggtitle("A") +
  theme_minimal()
)
```
**Cumulative proportion of tornadoes by maximum EF rating for all tornadoes (gray) and for tornadoes occurring in large clusters (black)**

Then to re-describe the histogram as log-cumulative odds, we need a series of intercept parameters. Each intercept will be on the log-cumulative-odds scale and stand in for the cumulative probability of each rating.
$$
\log \frac{\Pr(y_i \le k)}{1 - \Pr(y_i \le k)} = \alpha_k
$$
where $\alpha_k$ is an 'intercept' unique to each possible EF rating $k$. 

We compute these intercept parameters directly.
```{r}
( lco <- rethinking::logit(cum_pr_k) )
( lco_all <- rethinking::logit(cum_pr_k_all) )

plot.df$lco <- lco
plot.df$lco_all <- lco_all

(p2 <- ggplot(plot.df[1:5, ], aes(x = maxEF, y = lco_all)) +
  geom_point(color = "gray70") +
  geom_line(color = "gray70") +
  geom_point(aes(y = lco)) +
  geom_line(aes(y = lco)) +
  scale_y_continuous() +
  scale_x_continuous(minor_breaks = 0:4) +
  xlab("Maximum EF Rating") +
  ylab("Log-Cumulative-Odds") +
    ggtitle("B") +
  theme_minimal()
)
```
**Log-cumulative odds of a tornado by maximum EF rating for all tornadoes (gray) and for tornadoes occurring in large clusters (black)**

Note that the cumulative logit for the highest EF rating is infinity. This is because log(1/(1 - 1)) = $\infty$. This is always the case so we do not need a parameter for it. We get it for free from the law of total probability. So for $K$ = 6 possible maximum EF ratings we only need $K$ - 1 = 5 intercepts.

Combine into single figure.
```{r}
library(ggpubr)
ggarrange(p1, p2)
```

What we really want is the prior distribution of these intercepts. This allows us to take into account sample size and prior information, as well as insert predictor variables.

To use Bayes' theorem to compute the posterior distribution of these intercepts, we will need to compute the likelihood for each possible EF rating. So the last step in constructing the basic model fitting engine for these ordered categorical outcomes is to use cumulative probabilities $\Pr(y_i \le k)$ to compute the likelihood $\Pr(y_i = k)$.

```{r}
plot.df$ys <-  plot.df$cum_pr_k - plot.df$pr_k

ggplot(plot.df, aes(x = maxEF, y = cum_pr_k)) +
  geom_segment(aes(x = maxEF, xend = maxEF, y = ys, yend = cum_pr_k), size = 1.3, color = "gray70") +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1)) +
    xlab("Maximum EF Rating") +
    ylab("Cumulative Proportion") +
  theme_minimal()
```

Cumulative probability and ordered likelihood. The horizontal axis displays possible observable damage ratings, from 0 through 5. The vertical axis displays cumulative probability. The points show cumulative probability. These keep getting higher with each successive EF rating. The gray line segments show the discrete probability of each EF rating. These are the likelihoods that go into Bayes’ theorem.

In code form
$$
\begin{aligned} 
\hbox{R}_i &\sim \hbox{Ordered}(\mathbf{p}) \\ 
\hbox{logit}(p_k) &= \alpha_k \\
\alpha_k &\sim \hbox{Normal}(0, 10)
\end{aligned}
$$

The Ordered distribution is a categorical distribution that takes a vector $p = \{p_0, p_1, p_2, p_3, p_4\}$ of probabilities for each EF rating below the highest (EF5). Each response value $k$ in this vector is defined by its link to an intercept parameter ($\alpha_k$). 

### Adding a predictor variable

To include predictor variables, we define the log-cumulative-odds of each EF rating $k$ as a sum of its intercept $\alpha_k$ and a typical linear model. Suppose for  example we want to add a predictor $x$ to the model. We do this by defining a linear model $\phi_i = \beta x_i$. Then each cumulative logit becomes
$$
\begin{aligned}
\log \frac{\Pr(y_i \le k)}{1 - \Pr(y_i \le k)} &= \alpha_k - \phi_i \\
\phi_i &= \beta x_i
\end{aligned}
$$

The form ensures the correct ordering of the EF ratings while allowing for changes in the likelihood of each individual value as the predictor $x_i$ changes value. As the log-cumulative odds of every EF value ($k$) below the maximum decreases, the probability mass shifts upwards toward higher EF ratings. This is analogous to what happens in quantile regression (see \cite{JaggerElsner2009}).

Create scaled predictor variables.
```{r}
df2$Yrs <- scale(df2$Year)
df2$dists <- scale(df2$dist)
df2$CAPEs <- scale(df2$maxCAPE)
df2$HLCYs <- scale(df2$maxHLCY)
df2$BSs <- scale(df2$maxBS)
df2$CINs <- scale(df2$minCIN)
```

### Mixed effects model

Cluster number indexed from 1 to number of clusters.
```{r}
nT <- as.vector(table(df2$ID))
id <- rep(1:length(nT), times = nT)
df2$id <- id
```

Use the **brms** package.

Start by setting the family and the model formula. Get priors. Start with some default priors (for example "normal(0,5)" for the class = "b"). Sample from the priors and check the predictive distribution of the response. Adjust the priors accordingly.
```{r}
library(brms)
#load("fit.RData")
df2$maxEF1 <- df2$maxEF + 1

family <- brms::cumulative(threshold = "flexible")
formula <- maxEF1 ~ Yrs + dists + CAPEs + HLCYs + BSs + CINs  + (1|mo) + (1|id)

get_prior(formula, data = df2, family = family)

prior <- brm(formula = formula,
           data = df2,
           family = family,
           prior = c(set_prior("normal(0,.5)", class = "b"),
                    set_prior("student_t(3, 0, 10)", class = "Intercept"),
                    set_prior("student_t(3, 0, 10)", class = "sd")),
           sample_prior = "only",
           control = list(max_treedepth = 15),
           seed = 9112)

out <- predict(prior, probs = c(0, 1))
```

Fit the model.
```{r}
fit <- brm(formula = formula,
           data = df2,
           family = family,
           prior = c(set_prior("normal(0,.5)", class = "b"),
                    set_prior("student_t(3, 0, 10)", class = "Intercept"),
                    set_prior("student_t(3, 0, 10)", class = "sd")),
           control = list(max_treedepth = 15),
           seed = 9121)



save(fit, file = "fit.RData")
out2 <- predict(fit, probs = c(0, 1))
# stancode(post)

fit.df <- posterior_samples(fit)
```

Here we examine whether the effect of bulk shear is different for across different EF ratings.
```{r, eval=FALSE}
formula <- maxEF1 ~ Yrs + dists + CAPEs + HLCYs + cs(BSs) + CINs  + (1|id)
family <- "sratio"
fit3 <- brm(formula = formula,
            data = df2,
            family = family,
            prior = c(set_prior("normal(0,.55)", class = "b"),
                    set_prior("student_t(3, 0, 10)", class = "Intercept"),
                    set_prior("student_t(3, 0, 10)", class = "sd"),
                    set_prior("normal(-1, 2)", coef = "BSs")),
             seed = 9112)
```
Apparently not. The uncertainty bounds on the estimates overlap.

Cross validation. This takes a very long time.
```{r, eval=FALSE}
kfold1 <- kfold(fit, folds = "stratified", group = "id")
kfold <- kfold(fit, group = "id", chains = 1) 
```

```{r}
library(bayestestR)
equivalence_test(fit)
```

### Hypothesis test
```{r}
summary(fit)
```

Test the hypothesis that bulk shear is more important that CAPE.
```{r}
(hyp <- hypothesis(fit, "BSs > CAPEs", class = "b"))
plot(hyp)

hypothesis(fit, "BSs > 0", class = "b", group = "mo", scope = "coef")
```

#### Bulk shear

Consider how the model handles bulk shear. Set all variables to zero except distance to city and bulk shear. Set the distance to city to a small value (near the city). The scale and center parameters of the bulk shear variable are obtained and then extremes values are used.
```{r}
BSs <- scale(df2$maxBS)
scale <- attr(BSs, "scaled:scale")
center <- attr(BSs, "scaled:center")
( BSlo <- (15 - center) / scale )
( BShi <- (45 - center) / scale )
```

Line plots.
```{r}
dists <- -1.3
Yrs <- 0
CAPEs <- 0
HLCYs <- 0
BSs <- c(BSlo, BShi)
CINs <- 0

tpk <- numeric()
for ( s in 1:10 ) {
    p <- fit.df[s, ]
    ak <- as.numeric(p[1:5])
    phi <- p$b_BSs * BSs
    pk <- rethinking::pordlogit( 1:5 , a = ak , phi = phi )
    tpk <- rbind(tpk, pk)
}

tpk.df <- as.data.frame(tpk)
tpk.df$X <- rep(c(15, 45), times = 10)
tpk.df$S <- rep(1:10, each = 2)
names(tpk.df) <- c("EF0", "EF1", "EF2", "EF3", "EF4", "X", "S")

ggplot(tpk.df, aes(x = X, y = EF0, group = S)) +
  geom_line(color = "#EFF3FF") +
  geom_line(aes(y = EF1), color = "#BDD7E7") +
  geom_line(aes(y = EF2), color = "#6BAED6") +
  geom_line(aes(y = EF3), color = "#3182BD") +
  geom_line(aes(y = EF4), color = "#08519C") +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous() +
  xlab("Bulk Shear (m/s)") + ylab("Cumulative Proportion") +
  theme_minimal()
```

Area plots.
```{r}
#p <- fit.df[1, ]
p <- colMeans(fit.df)

ak <- as.numeric(p[1:5])
phi <- p[10] * BSs[1]
dk1 <- rethinking::dordlogit( 1:5 , a = ak , phi = phi )

phi <- p[10] * BSs[2]
dk2 <- rethinking::dordlogit( 1:5 , a = ak , phi = phi )

dk <- c(dk1, dk2)

tpk.df <- data.frame(dk,
                     BS = rep(c(15, 45), each = 5),
                     EF = factor(rep(c("0", "1", "2", "3", "4"), times = 2), 
                                 levels = c("4", "3", "2", "1", "0")))

ggplot(tpk.df, aes(x = BS, y = dk)) + 
  geom_area(aes(fill = EF), color = "white") +
  scale_fill_brewer(type = "seq", direction = -1) +
  xlab("Bulk Shear (m/s)") + ylab("Cumulative Proportion") +
  theme_minimal() +
  ggtitle(label = "Predicted proportion of tornadoes by EF category") 
```

Marginal plot. Bulk shear.
```{r}
gg <- marginal_effects(fit, categorical = TRUE)
ggBS <- gg[[5]]
levels(ggBS$cats__) <- c("0", "1", "2", "3", "4", "5")

BSs <- scale(df2$maxBS)
scale <- attr(BSs, "scaled:scale")
center <- attr(BSs, "scaled:center")

ggBS <- ggBS %>%
  mutate(BS = BSs * scale + center)

(p1 <- ggplot(ggBS, aes(x = BS, y = estimate__, col = cats__)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = cats__), 
              alpha = .7, col = NA) +
  scale_color_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  scale_fill_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  scale_y_continuous(limits = c(0, 1)) +
  ylab("Relative Chance of a Tornado\nCausing EF-level Damage") +
  xlab("Bulk Shear (m/s)") +
    ggtitle("A") +
  theme_minimal()
)
```

Marginal plot. Distance to nearest city.
```{r}
ggDist <- gg[[2]]
levels(ggDist$cats__) <- c("0", "1", "2", "3", "4", "5")

dists <- scale(df2$dist)
scale <- attr(dists, "scaled:scale")
center <- attr(dists, "scaled:center")

ggDist <- ggDist %>%
  mutate(dist = dists * scale + center)

ggplot(ggDist, aes(x = dist/1000, y = estimate__, col = cats__)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = cats__), 
              alpha = .7, col = NA) +
  scale_color_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  scale_fill_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  ylab("Relative Chance of a Tornado\nCausing EF-level Damage") +
  xlab("Distance to nearest city (km)") +
  theme_minimal()
```

Marginal plot. CAPE.
```{r}
ggCAPE <- gg[[3]]
levels(ggCAPE$cats__) <- c("0", "1", "2", "3", "4", "5")

CAPEs <- scale(df2$maxCAPE)
scale <- attr(CAPEs, "scaled:scale")
center <- attr(CAPEs, "scaled:center")

ggCAPE <- ggCAPE %>%
  mutate(CAPE = CAPEs * scale + center)

(p2 <- ggplot(ggCAPE, aes(x = CAPE, y = estimate__, col = cats__)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = cats__), 
              alpha = .7, col = NA) +
  scale_color_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  scale_fill_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  scale_y_continuous(limits = c(0, 1)) +
  ylab("Relative Chance of a Tornado\nCausing EF-level Damage") +
  xlab("CAPE (J/kg)") +
    ggtitle("B") +
  theme_minimal()
)
```

```{r}
ggarrange(p1, p2)
```
**Marginal effect of bulk shear (A) and CAPE (B) on the distribution of tornadoes by maximum EF rating.**


