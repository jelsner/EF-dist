---
title: "Forecasting tornado count distributions by EF damage rating"
author: "James Elsner"
date: "2/14/2019"
output: github_notebook
editor_options:
  chunk_output_type: console
---
Abstract for the European SLS conference in Krakow, POLAND, 4-8 November 2019.

Abstract Deadline: April 24, 2019
Submission link: https://meetings.copernicus.org/ecss2019/abstract_management/

Predicting tornado count distributions by EF rating

Elsner/Schroder

_Journal of Applied Meteorology and Climatology_

The science of forecasting where an outbreak of severe convective weather might occur days in advance is well developed. For example, considerable skill exists in outlining what regions of the United States have the highest risk of severe weather on any given day. But more work is needed to anticipate specific characteristics of the outbreaks. Here we develop a statistical model that can be used to predict the probability distribution of tornado counts by EF rating on days expected to produce an outbreak. The model is fit using population centers to condition on the fact that damage ratings are related to the number (and type) of potential targets. The model quantifies the importance of this demographic information relative to information from environmental factors known to influence tornadoes, including CAPE and bulk shear. The model controls for distance to nearest city to quantify the percent increase in the chance of at least one tornado rated EF3 or higher for a unit increase in CAPE and bulk shear. The model is an ordered logistic regression and the coefficients are determined using the method of Hamiltonian Monte Carlo with the Stan language. Stan code is generated from R through the 'brms' package. The flexibility of this approach makes it easy to adjust the model to forecast other outbreak characteristics and to include domain-specific knowledge.

Motivation: Current lack of statistical guidance for forecasting outbreak characteristics. Is this true? Perhaps such tools can be used to better describe the threat and thus hopefully save lives.

Ordered logistic regression to predict the probability distributions of tornado counts by EF rating. With distance to nearest city as a predictor we will be able to see how population shifts the distributions for example from EF0 to EF1 from EF1 to EF2. We know that the chance of getting an EF4+ tornado in the data set increases with the number of structures. What we don't know is by how much relative to an EF3.

The study aims to show: (1) the feasibility of forecasting a particular outbreak characteristic, and (2) a flexible modeling framework that can be used to forecast other characteristics.

Load packages.
```{r}
library(sf)
library(tmap)
library(dplyr)
library(ggplot2)
```

Load the data. The study uses the outbreak data collated in the `tor-cluster` GitHub project. The methodology for determining the big outbreak days is detailed in Schroder and Elsner (2019). Briefly, the method groups tornadoes occurring in the period 1994 through 2017 based on space-time distances between genesis locations. If two tornadoes occur close together in space and time, then they are grouped together. Grouping stops when the the space-time distance threshold exceeds 50K meter-seconds.
```{r}
load("~/Desktop/Projects/EF-dist/BigDays.RData")
st_crs(Torn.sfT)
st_crs(BigDays.sfdfT)
```

The simple feature `BigDays.sfdfT` contains outbreak-level data including environmental variables. The simple feature `Torn.sfT` contains tornado-level data for all tornadoes on the big days. Note: they have different CRSs.

A map of one tornado outbreak.
```{r}
library(USAboundaries)
ctys <- us_counties()
sts1 <- us_states(states = c("IL", "IN", "KY", "MO", "KS",
                            "OK", "AR", "TN", "MS", "TX"))
sts <- us_states()

groupNo <- 53

BD <- BigDays.sfdfT %>%
  filter(groupNumber == groupNo)
BDt <- Torn.sfT %>%
  filter(groupNumber == groupNo) %>%
  mutate(magF = factor(mag, levels = 0:4))

library(tmap)

tm_shape(BD) +
  tm_polygons() +
tm_shape(sts1, is.master = TRUE) +
  tm_borders() +
tm_shape(sts) +
  tm_borders(col = "gray70") +
  tm_text("name", size = .6) +
tm_shape(BDt) +
  tm_dots(size = .5) +
tm_compass(position = c("left", "bottom")) +
tm_scale_bar(position = c("left", "bottom")) +
tm_layout(title = "April 26, 1994")
```

Distribution of outbreaks by month.
```{r}
ggplot(BigDays.sfdfT, aes(x = as.factor(Month))) +
  geom_bar(fill = "gray70") +
  scale_x_discrete(labels = month.name) +
  scale_y_continuous(limits = c(0, NA)) +
  ylab("Number of Outbreaks\nWith At Least 10 Tornadoes") + xlab("") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(BigDays.sfdfT, aes(x = Month)) +
  geom_bar(fill = "gray70") +
  scale_x_continuous(breaks = seq(1, 12, 1), labels = month.abb) +
  scale_y_continuous(limits = c(0, NA)) +
  coord_polar(start = 0) +
  labs(x = "Month", y = "Number of Outbreaks\nWith At Least 10 Tornadoes") +
  theme_minimal()

ggplot(BigDays.sfdfT, aes(x = nT)) +
  geom_histogram(fill = "gray70", binwidth = 5) +
  scale_y_continuous(limits = c(0, NA)) +
  ylab("Number of Outbreaks") + xlab("Number of Tornadoes") +
  theme_minimal() 
```

We join the environmental variables at the outbreak-level with the data at the tornado level.
```{r}
df <- as.data.frame(BigDays.sfdfT) %>%
  select(ID, maxCAPE, maxHLCY, minCIN, maxBS, maxSM) %>%
  left_join(as.data.frame(Torn.sfT), by = "ID")
```

Compute the distance to nearest city/town.
```{r}
library(USAboundaries)

C.sf <- us_cities() %>%
  st_transform(crs = 102004)

dist <- numeric()
for(i in seq_len(nrow(Torn.sfT))){
  dist[i] <- min(st_distance(Torn.sfT[i, ], C.sf))
}

df$dist <- dist
```

Returns the closest cities as a data frame.
```{r}
closest.df <- numeric()
 for(i in seq_len(nrow(Torn.sfT))){
     closest.df <- rbind(closest.df, as.data.frame(C.sf[which.min(st_distance(C.sf, Torn.sfT[i, ])), ]))
 }

closest.df %>%
  group_by(stplfips_2010) %>%
  summarize(nT = n()) %>%
  arrange(desc(nT))

closest.df %>%
  filter(stplfips_2010 == 2018250)
closest.df %>%
  filter(stplfips_2010 == 2063600)
```

Dodge City KS (2010 population of 27,340) and Scott City KS (3,816) are the two cities with the most 'nearby' tornadoes.

### Rethinking package

McElreath Lecture 14. Chapter 12 of his rethinking book.

Load the **rethinking** package.
```{r}
#devtools::install_github("rmcelreath/rethinking")
#library(rethinking)
```

### Describing the ordered distribution of maximum EF ratings with intercepts

We begin with a histogram of maximum EF rating per tornado.
```{r}
df2 <- df %>%
  mutate(maxEF = as.integer(mag)) %>%
  select(ID, maxEF, dist, maxEF, Year, mo, maxCAPE, maxHLCY, maxBS, minCIN)

plot.df <- data.frame(table(df2$maxEF))
names(plot.df) <- c("EF", "Frequency")

ggplot(plot.df, aes(x = EF, y = Frequency)) + 
  geom_point() + 
  geom_segment(aes(xend = EF, yend = 0)) +
  xlab("Maximum EF Rating") +
  theme_minimal()

table(df2$maxEF)/dim(df2)[1]
load("~/Desktop/Projects/EF-dist/BigDaysInLargeGroups.RData") # Note the file BigDays.sfdfT now is only days within a large group
table(All_Tornadoes$mag)/nrow(All_Tornadoes)
```

As expected the histogram of tornadoes by EF rating on big outbreak days shows that the vast majority rated EF0 or EF1 and far fewer rated EF4 or EF5. But relative to all tornadoes the distribution favors higher ratings. For example 3.5\% of big outbreak day tornadoes are rated EF3 compared with 2.3\% of all tornadoes. And .08\% of big outbreak day tornadoes are rated EF5 compared with .05\% of all tornadoes. We describe this histogram on the log-cumulative-odds scale by constructing the odds of a cumulative probability and then taking logarithms. Since the logit is log-odds, the cumulative logit is log-cumulative-odds. Both the logit and cumulative logit constrain the probabilities to the interval between 0 and 1. 

When we add predictor variables, we can safely do so on the cumulative logit scale. The link function takes care of converting the parameter estimates to the proper probability scale.

We first compute the cumulative probabilities from the histogram. The discrete proportion of each EF ranking.
```{r}
pr_k <- as.vector(table(df2$maxEF) / nrow(df2))
pr_k_all <- as.vector(table(All_Tornadoes$mag) / nrow(All_Tornadoes))
cum_pr_k <- cumsum(pr_k)
cum_pr_k_all <- cumsum(pr_k_all)

plot.df <- data.frame(maxEF = 0:5, pr_k, cum_pr_k, cum_pr_k_all)

ggplot(plot.df, aes(x = maxEF, y = cum_pr_k_all)) +
  geom_point(color = "gray70") +
  geom_line(color = "gray70") +
  geom_point(aes(y = cum_pr_k)) +
  geom_line(aes(y = cum_pr_k)) +
  scale_y_continuous(limits = c(0, 1)) +
    xlab("Maximum EF Rating") +
    ylab("Cumulative Proportion") +
  theme_minimal()
```
**Cumulative proportion of tornadoes by maximum EF rating for all tornadoes (gray) and for tornadoes occurring in big outbreaks (black)**

Then to re-describe the histogram as log-cumulative odds, we need a series of intercept parameters. Each intercept will be on the log-cumulative-odds scale and stand in for the cumulative probability of each rating.
$$
\log \frac{\Pr(y_i \le k)}{1 - \Pr(y_i \le k)} = \alpha_k
$$
where $\alpha_k$ is an 'intercept' unique to each possible EF rating $k$. 

We compute these intercept parameters directly.
```{r}
( lco <- rethinking::logit(cum_pr_k) )
( lco_all <- rethinking::logit(cum_pr_k_all) )

plot.df$lco <- lco
plot.df$lco_all <- lco_all

ggplot(plot.df[1:5, ], aes(x = maxEF, y = lco_all)) +
  geom_point(color = "gray70") +
  geom_line(color = "gray70") +
  geom_point(aes(y = lco)) +
  geom_line(aes(y = lco)) +
  scale_y_continuous() +
  xlab("Maximum EF Rating") +
  ylab("log-cumulative-odds") +
  theme_minimal()
```

Note that the cumulative logit for the highest EF rating is infinity. This is because log(1/(1 - 1)) = $\infty$. This is always the case so we do not need a parameter for it. We get it for free from the law of total probability. So for $K$ = 6 possible maximum EF ratings we only need $K$ - 1 = 5 intercepts.

What we really want is the prior distribution of these intercepts. This allows us to take into account sample size and prior information, as well as insert predictor variables.

To use Bayes' theorem to compute the posterior distribution of these intercepts, we will need to compute the likelihood for each possible EF rating. So the last step in constructing the basic model fitting engine for these ordered categorical outcomes is to use cumulative probabilities $\Pr(y_i \le k)$ to compute the likelihood $\Pr(y_i = k)$.

```{r}
plot.df$ys <-  plot.df$cum_pr_k - plot.df$pr_k

ggplot(plot.df, aes(x = maxEF, y = cum_pr_k)) +
  geom_segment(aes(x = maxEF, xend = maxEF, y = ys, yend = cum_pr_k), size = 1.3, color = "gray70") +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1)) +
    xlab("Maximum EF Rating") +
    ylab("Cumulative Proportion") +
  theme_minimal()
```

Cumulative probability and ordered likelihood. The horizontal axis displays possible observable damage ratings, from 0 through 5. The vertical axis displays cumulative probability. The points show cumulative probability. These keep getting higher with each successive EF rating. The gray line segments show the discrete probability of each EF rating. These are the likelihoods that go into Bayes’ theorem.

In code form
$$
\begin{aligned} 
\hbox{R}_i &\sim \hbox{Ordered}(\mathbf{p}) \\ 
\hbox{logit}(p_k) &= \alpha_k \\
\alpha_k &\sim \hbox{Normal}(0, 10)
\end{aligned}
$$

The Ordered distribution is a categorical distribution that takes a vector $p = \{p_0, p_1, p_2, p_3, p_4\}$ of probabilities for each EF rating below the highest (EF5). Each response value $k$ in this vector is defined by its link to an intercept parameter ($\alpha_k$). 

### Adding a predictor variable

To include predictor variables, we define the log-cumulative-odds of each EF rating $k$ as a sum of its intercept $\alpha_k$ and a typical linear model. Suppose for  example we want to add a predictor $x$ to the model. We do this by defining a linear model $\phi_i = \beta x_i$. Then each cumulative logit becomes
$$
\begin{aligned}
\log \frac{\Pr(y_i \le k)}{1 - \Pr(y_i \le k)} &= \alpha_k - \phi_i \\
\phi_i &= \beta x_i
\end{aligned}
$$

The form automatically ensures the correct ordering of the EF ratings, while still morphing the likelihood of each individual value as the predictor $x_i$ changes value. Why is the linear model $\phi$ subtracted from each intercept? Because if we decrease the log-cumulative odds of every outcome value $k$ below the maximum, this necessarily shifts the probability mass upwards towards higher outcome values. Recall quantile regression.

Create scaled predictor variables.
```{r}
df2$Yrs <- scale(df2$Year)
df2$dists <- scale(df2$dist)
df2$CAPEs <- scale(df2$maxCAPE)
df2$HLCYs <- scale(df2$maxHLCY)
df2$BSs <- scale(df2$maxBS)
df2$CINs <- scale(df2$minCIN)
```

### Mixed effects model

Cluster number indexed from 1 to number of clusters.
```{r}
nT <- as.vector(table(df2$ID))
id <- rep(1:length(nT), times = nT)
df2$id <- id
```

Use the **brms** package.
```{r}
library(brms)
#load("fit.RData")
df2$maxEF1 <- df2$maxEF + 1

formula <- maxEF1 ~ Yrs + dists + CAPEs + HLCYs + BSs + CINs  + (1|id)
family <- brms::cumulative(threshold = "flexible")

get_prior(formula, data = df2, family = family)

fit <- brm(formula = formula,
           data = df2,
           family = family,
           prior = c(set_prior("normal(0,5)", class = "b"),
                    set_prior("student_t(3, 0, 10)", class = "Intercept"),
                    set_prior("student_t(3, 0, 10)", class = "sd")),
           seed = 9112)

formula <- maxEF1 ~ Yrs + dists + CAPEs + HLCYs + BSs + CINs  + (1|mo) + (1|id)
fit2 <- brm(formula = formula,
           data = df2,
           family = family,
           prior = c(set_prior("normal(0,5)", class = "b"),
                    set_prior("student_t(3, 0, 10)", class = "Intercept"),
                    set_prior("student_t(3, 0, 10)", class = "sd")),
           control = list(max_treedepth = 15),
           seed = 9112)

save(fit, file = "fit.RData")
save(fit2, file = "fit2.RData")
#out <- predict(post, probs = c(0, 1))
# stancode(post)

fit.df <- posterior_samples(fit)
```

Here we examine whether the effect of bulk shear is different for across different EF ratings.
```{r, eval=FALSE}
formula <- maxEF1 ~ Yrs + dists + CAPEs + HLCYs + cs(BSs) + CINs  + (1|id)
family <- "sratio"
fit3 <- brm(formula = formula,
            data = df2,
            family = family,
            prior = c(set_prior("normal(0,5)", class = "b"),
                    set_prior("student_t(3, 0, 10)", class = "Intercept"),
                    set_prior("student_t(3, 0, 10)", class = "sd"),
                    set_prior("normal(-1, 2)", coef = "BSs")),
             seed = 9112)
```
Apparently not. The uncertainty on the estimates overlap.

Cross validation. This takes a long time.
```{r, eval=FALSE}
kfold1 <- kfold(fit, folds = "stratified", group = "id")
kfold <- kfold(fit, group = "id", chains = 1) 
```

```{r}
library(bayestestR)
equivalence_test(fit)
```

#### Bulk shear

Consider how the model handles bulk shear. Set all variables to zero except distance to city and bulk shear. Set the distance to city to a small value (near the city). The scale and center parameters of the bulk shear variable are obtained and then extremes values are used.
```{r}
BSs <- scale(df2$maxBS)
scale <- attr(BSs, "scaled:scale")
center <- attr(BSs, "scaled:center")
( BSlo <- (15 - center) / scale )
( BShi <- (45 - center) / scale )
```

Line plots.
```{r}
dists <- -1.3
Yrs <- 0
CAPEs <- 0
HLCYs <- 0
BSs <- c(BSlo, BShi)
CINs <- 0

tpk <- numeric()
for ( s in 1:10 ) {
    p <- fit.df[s, ]
    ak <- as.numeric(p[1:5])
    phi <- p$b_BSs * BSs
    pk <- rethinking::pordlogit( 1:5 , a = ak , phi = phi )
    tpk <- rbind(tpk, pk)
}

tpk.df <- as.data.frame(tpk)
tpk.df$X <- rep(c(15, 45), times = 10)
tpk.df$S <- rep(1:10, each = 2)
names(tpk.df) <- c("EF0", "EF1", "EF2", "EF3", "EF4", "X", "S")

ggplot(tpk.df, aes(x = X, y = EF0, group = S)) +
  geom_line(color = "#EFF3FF") +
  geom_line(aes(y = EF1), color = "#BDD7E7") +
  geom_line(aes(y = EF2), color = "#6BAED6") +
  geom_line(aes(y = EF3), color = "#3182BD") +
  geom_line(aes(y = EF4), color = "#08519C") +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous() +
  xlab("Bulk Shear (m/s)") + ylab("Cumulative Proportion") +
  theme_minimal()
```

Area plots.
```{r}
#p <- fit.df[1, ]
p <- colMeans(fit.df)

ak <- as.numeric(p[1:5])
phi <- p[10] * BSs[1]
dk1 <- rethinking::dordlogit( 1:5 , a = ak , phi = phi )

phi <- p[10] * BSs[2]
dk2 <- rethinking::dordlogit( 1:5 , a = ak , phi = phi )

dk <- c(dk1, dk2)

tpk.df <- data.frame(dk,
                     BS = rep(c(15, 45), each = 5),
                     EF = factor(rep(c("0", "1", "2", "3", "4"), times = 2), 
                                 levels = c("4", "3", "2", "1", "0")))

ggplot(tpk.df, aes(x = BS, y = dk)) + 
  geom_area(aes(fill = EF), color = "white") +
  scale_fill_brewer(type = "seq", direction = -1) +
  xlab("Bulk Shear (m/s)") + ylab("Cumulative Proportion") +
  theme_minimal() +
  ggtitle(label = "Predicted proportion of tornadoes by EF category") 
```

Marginal plot. Bulk shear.
```{r}
gg <- marginal_effects(fit, categorical = TRUE)
ggBS <- gg[[5]]
levels(ggBS$cats__) <- c("0", "1", "2", "3", "4", "5")

BSs <- scale(df2$maxBS)
scale <- attr(BSs, "scaled:scale")
center <- attr(BSs, "scaled:center")

ggBS <- ggBS %>%
  mutate(BS = BSs * scale + center)

ggplot(ggBS, aes(x = BS, y = estimate__, col = cats__)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = cats__), 
              alpha = .7, col = NA) +
  scale_color_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  scale_fill_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  ylab("Relative Chance of a Tornado\nCausing EF-level Damage") +
  xlab("Bulk Shear (m/s)") +
  theme_minimal()
```

Marginal plot. Distance to nearest city.
```{r}
ggDist <- gg[[2]]
levels(ggDist$cats__) <- c("0", "1", "2", "3", "4", "5")

dists <- scale(df2$dist)
scale <- attr(dists, "scaled:scale")
center <- attr(dists, "scaled:center")

ggDist <- ggDist %>%
  mutate(dist = dists * scale + center)

ggplot(ggDist, aes(x = dist/1000, y = estimate__, col = cats__)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = cats__), 
              alpha = .7, col = NA) +
  scale_color_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  scale_fill_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  ylab("Relative Chance of a Tornado\nCausing EF-level Damage") +
  xlab("Distance to nearest city (km)") +
  theme_minimal()
```



OLD STUFF

We fit this model by adding the slope parameters and predictor variables to the `phi` parameter inside `dordlogit()` function. Note this model ignores the group-level effect of the outbreaks.
```{r}
m1.2 <- map(
  alist(
    maxEF1 ~ dordlogit( phi, c(a0, a1, a2, a3, a4) ),
    phi <- bYr * Yrs + bDis * dists + bCA * CAPEs + bH * HLCYs + bS * BSs + bCI * CINs,
    a[id] ~ dnorm(0, 1.5),
    bYr ~ dnorm(0, 10),
    bDis ~ dnorm(0, 10),
    bCA ~ dnorm(0, 10),
    bH ~ dnorm(0, 10),
    bS ~ dnorm(0, 10),
    bCI ~ dnorm(0, 10),
#    bI ~ dnorm(0, 10),
    c(a0, a1, a2, a3, a4) <- dnorm(0, 10)
  ) ,
  data = df2 ,
  start = list(a0 = -1, a1 = 0, a2 = 1, a3 = 2, a4 = 3, 
               bYr = 0, bDis = 0, bCA = 0, bH = 0, bS = 0, bCI = 0)
)
```

Model summary.
```{r}
summary(m1.2)
```

### Plot the posterior predictions

Extract the posterior values of the parameters.
```{r}
post.df <- extract.samples(m1.2)
```

Line plots.
```{r}
dists <- -1.3
Yrs <- 0
CAPEs <- 0
HLCYs <- 0
BSs <- c(BSlo, BShi)
CINs <- 0

tpk <- numeric()
for ( s in 1:10 ) {
    p <- post.df[s, ]
    ak <- as.numeric(p[1:5])
    phi <- p$bS * BSs
    pk <- pordlogit( 1:5 , a = ak , phi = phi )
    tpk <- rbind(tpk, pk)
}

tpk.df <- as.data.frame(tpk)
tpk.df$X <- rep(c(15, 45), times = 10)
tpk.df$S <- rep(1:10, each = 2)
names(tpk.df) <- c("EF0", "EF1", "EF2", "EF3", "EF4", "X", "S")

ggplot(tpk.df, aes(x = X, y = EF0, group = S)) +
  geom_line(color = "#EFF3FF") +
  geom_line(aes(y = EF1), color = "#BDD7E7") +
  geom_line(aes(y = EF2), color = "#6BAED6") +
  geom_line(aes(y = EF3), color = "#3182BD") +
  geom_line(aes(y = EF4), color = "#08519C") +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous() +
  xlab("Bulk Shear (m/s)") + ylab("Cumulative Proportion") +
  theme_minimal()
```

Area plots.
```{r}
#p <- post.df[1, ]
p <- colMeans(post.df)

ak <- as.numeric(p[1:5])
phi <- p[10] * BSs[1]
dk1 <- dordlogit( 1:5 , a = ak , phi = phi )

phi <- p[10] * BSs[2]
dk2 <- dordlogit( 1:5 , a = ak , phi = phi )

dk <- c(dk1, dk2)

tpk.df <- data.frame(dk,
                     BS = rep(c(15, 45), each = 5),
                     EF = factor(rep(c("0", "1", "2", "3", "4"), times = 2), 
                                 levels = c("4", "3", "2", "1", "0")))

ggplot(tpk.df, aes(x = BS, y = dk)) + 
  geom_area(aes(fill = EF), color = "white") +
  scale_fill_brewer(type = "seq", direction = -1) +
  xlab("Bulk Shear (m/s)") + ylab("Cumulative Proportion") +
  theme_minimal() +
  ggtitle(label = "Predicted proportion of tornadoes by EF category") 

```

#### Distance to city

Consider how the model handles distance from city. Set all variables to zero except distance from city and bulk shear. Set bulk shear to a high value. The scale and center parameters of the bulk shear variable are obtained and then extremes values are used.
```{r}
dists <- scale(df2$dist)
scale <- attr(dists, "scaled:scale")
center <- attr(dists, "scaled:center")

( Dclose <- (0 - center) / scale )
( Dfar <- (100000 - center) / scale )
```

Line plots.
```{r}
dists <- c(Dclose, Dfar)
Yrs <- 0
CAPEs <- 0
HLCYs <- 0
BSs <- 2.2
CINs <- 0

tpk <- numeric()
for ( s in 1:10 ) {
    p <- post.df[s, ]
    ak <- as.numeric(p[1:5])
    phi <- p$bS * BSs + p$bDis * dists
    pk <- pordlogit( 1:5 , a = ak , phi = phi )
    tpk <- rbind(tpk, pk)
}

tpk.df <- as.data.frame(tpk)
tpk.df$X <- rep(c(0, 100000), times = 10)
tpk.df$S <- rep(1:10, each = 2)
names(tpk.df) <- c("EF0", "EF1", "EF2", "EF3", "EF4", "X", "S")

ggplot(tpk.df, aes(x = X, y = EF0, group = S)) +
  geom_line(color = "#EFF3FF") +
  geom_line(aes(y = EF1), color = "#BDD7E7") +
  geom_line(aes(y = EF2), color = "#6BAED6") +
  geom_line(aes(y = EF3), color = "#3182BD") +
  geom_line(aes(y = EF4), color = "#08519C") +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous() +
  xlab("Distance from city (m)") + ylab("Cumulative Proportion") +
  theme_minimal()
```

Area plots.
```{r}
#p <- post.df[1, ]
p <- colMeans(post.df)
ak <- as.numeric(p[1:5])
phi <-  p[10] * BSs + p[7] * dists[1]
dk1 <- dordlogit( 1:5 , a = ak , phi = phi )

phi <- p[10] * BSs + p[7] * dists[2]
dk2 <- dordlogit( 1:5 , a = ak , phi = phi )

dk <- c(dk1, dk2)

tpk.df <- data.frame(dk,
                     Dc = rep(c(0, 100000), each = 5),
                     EF = factor(rep(c("0", "1", "2", "3", "4"), times = 2), 
                                 levels = c("4", "3", "2", "1", "0")))

ggplot(tpk.df, aes(x = Dc/1000, y = dk)) + 
  geom_area(aes(fill = EF), color = "white") +
  scale_fill_brewer(type = "seq", direction = -1) +
  xlab("Distance from nearest city/town (km)") + ylab("Cumulative Proportion") +
  theme_minimal() +
  ggtitle(label = "Predicted proportion of tornadoes by EF category") 

```


