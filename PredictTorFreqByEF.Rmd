---
title: "Forecasting tornado count distributions by EF damage rating"
author: "James Elsner"
date: "2/14/2019"
output: github_notebook
editor_options:
  chunk_output_type: console
---
Abstract for the European SLS conference in Krakow, POLAND, 4-8 November 2019.

Abstract Deadline: April 24, 2019
Submission link: https://meetings.copernicus.org/ecss2019/abstract_management/

Predicting tornado count distributions by EF rating

Elsner/Schroder

_Journal of Applied Meteorology and Climatology_

Load packages.
```{r}
library(sf)
library(tmap)
library(dplyr)
library(ggplot2)
```

Load the data. The study uses the data collated in the `tor-cluster` GitHub project. The method for determining the big days is detailed in Schroder and Elsner (2019). Briefly, it groups tornadoes occurring in the period 1994 through 2017 based on space-time distances between genesis locations. If two tornadoes occur close in space and time, they are grouped together. The procedure stops when the the space-time distance threshold exceeds 50K meter-seconds.
```{r}
load("~/Desktop/Projects/EF-dist/BigDays.RData")
st_crs(Torn.sfT)
st_crs(BigDays.sfdfT)

#load("~/Desktop/Projects/EF-dist/BigDays2.RData")
```

The simple feature `BigDays.sfdfT` contains group-level data including environmental variables. The simple feature `Torn.sfT` contains tornado-level data for all tornadoes on the big days. Note: they have different CRSs.

A map of one tornado group used in this study.
```{r}
library(USAboundaries)
ctys <- us_counties()
sts1 <- us_states(states = c("IL", "IN", "KY", "MO", "KS",
                            "OK", "AR", "TN", "MS", "TX"))
sts <- us_states()

groupNo <- 53

BD <- BigDays.sfdfT %>%
  filter(groupNumber == groupNo)
BDt <- Torn.sfT %>%
  filter(groupNumber == groupNo) %>%
  mutate(magF = factor(mag, levels = 0:4))

library(tmap)

tm_shape(BD) +
  tm_polygons() +
tm_shape(sts1, is.master = TRUE) +
  tm_borders() +
tm_shape(sts) +
  tm_borders(col = "gray70") +
  tm_text("name", size = .6) +
tm_shape(BDt) +
  tm_dots(size = .5) +
tm_compass(position = c("left", "bottom")) +
tm_scale_bar(position = c("left", "bottom")) +
tm_layout(title = "April 26, 1994")
```
### Figure: Map of one tornado group used in this study.

Distribution of tornado groups by month.
```{r}
ggplot(BigDays.sfdfT, aes(x = as.factor(Month))) +
  geom_bar(fill = "gray70") +
  scale_x_discrete(labels = month.name) +
  scale_y_continuous(limits = c(0, NA)) +
  ylab("Number of Tornado Groups\nWith At Least 10 Tornadoes") + xlab("") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

( p0 <- ggplot(BigDays.sfdfT, aes(x = Mo)) +
  geom_bar(fill = "gray70") +
  scale_x_continuous(breaks = seq(1, 12, 1), labels = month.abb) +
  scale_y_continuous(limits = c(0, NA)) +
  coord_polar(start = 0) +
  labs(x = "Month", y = "Number of Tornado Groups\nWith At Least 10 Tornadoes") +
  theme_minimal() )

ggplot(BigDays.sfdfT, aes(x = nT)) +
  geom_histogram(fill = "gray70", binwidth = 5) +
  scale_y_continuous(limits = c(0, NA)) +
  ylab("Number of Groups") + xlab("Number of Tornadoes") +
  theme_minimal() 
```
### Figure: Monthly distribution of big groups.

We join the environmental variables at the group-level with the data at the tornado level.

Given a group of at least ten tornadoes within a single convective 24-hour period, we extract environmental variables from the North American Regional Reanalysis (NARR) obtained from the the National Center for Atmospheric Research (NCAR). Variables are available on a 32.4 km grid and represent of a blend of model and observational sounding data.

We use the 3-hourly files that contain environmental data for each day ranging from 0Z to 21Z in 3-hour increments. For each tornado group, we calculate the closest 3-hour time before the occurrence of the first tornado. We pick a time before the event starts because we want to sample the pre-storm environment. We use tornado groups occurring between January 1994 and December 2017 resulting in a total of 16,504 tornadoe in 212 groups.
```{r}
df <- as.data.frame(BigDays.sfdfT) %>%
  select(ID, maxCAPE, maxHLCY, minCIN, maxBS, maxSM) %>%
  left_join(as.data.frame(Torn.sfT), by = "ID")
```

For each tornado we compute the distance to the nearest city/town. Distance-to-nearest city/town serves as a proxy for the potential number of damage targets. Tornadoes occurring close to towns or cities (short distances) will, on average, have a much higher number of nearby targets.
```{r}
library(USAboundaries)

C.sf <- us_cities() %>%
  st_transform(crs = 102004)

dist <- numeric()
for(i in seq_len(nrow(Torn.sfT))){
  dist[i] <- min(st_distance(Torn.sfT[i, ], C.sf))
}

df$dist <- dist
```

Returns the closest cities as a data frame.
```{r}
closest.df <- numeric()
 for(i in seq_len(nrow(Torn.sfT))){
     closest.df <- rbind(closest.df, as.data.frame(C.sf[which.min(st_distance(C.sf, Torn.sfT[i, ])), ]))
 }

closest.df %>%
  group_by(stplfips_2010) %>%
  summarize(nT = n()) %>%
  arrange(desc(nT))

closest.df %>%
  filter(stplfips_2010 == 2018250)
closest.df %>%
  filter(stplfips_2010 == 2063600)
```

Dodge City KS (2010 population of 27,340) and Scott City KS (3,816) are the two cities with the most 'nearby' tornadoes.

### Rethinking package

McElreath Lecture 14. Chapter 12 of his rethinking book.

Load the **rethinking** package.
```{r}
#devtools::install_github("rmcelreath/rethinking")
library(rethinking)
```

### Describing the ordered distribution of maximum EF ratings with intercepts

We begin with a histogram of maximum EF rating per tornado.
```{r}
df2 <- df %>%
  mutate(maxEF = as.integer(mag)) %>%
  select(ID, maxEF, dist, maxEF, Year, mo, maxCAPE, maxHLCY, maxBS, minCIN)

plot.df <- data.frame(table(df2$maxEF))
names(plot.df) <- c("EF", "Frequency")

ggplot(plot.df, aes(x = EF, y = Frequency)) + 
  geom_point() + 
  geom_segment(aes(xend = EF, yend = 0)) +
  xlab("Maximum EF Rating") +
  theme_minimal()

table(df2$maxEF)/dim(df2)[1]
load("~/Desktop/Projects/EF-dist/BigDaysInLargeGroups.RData") # Note the file BigDays.sfdfT now is only days within a large group
table(All_Tornadoes$mag)/nrow(All_Tornadoes)
```
### Figure: Histogram of tornadoes by maximum EF rating. Only tornadoes occurring in large clusters are considered (see text).

As expected the histogram by EF rating on large cluster days shows that the vast majority of tornadoes are rated EF0 or EF1 with far fewer rated EF4 or EF5. But relative to all tornadoes the distribution favors higher ratings. For example, 3.5\% of big outbreak day tornadoes are rated EF3 compared with 2.3\% of all tornadoes. And .08\% of big outbreak day tornadoes are rated EF5 compared with .05\% of all tornadoes. 

Next we describe this histogram on the log-cumulative-odds scale by constructing the odds of a cumulative probability and then taking logarithms. Since the logit is log-odds, the cumulative logit is log-cumulative-odds. Both the logit and cumulative logit constrain the probabilities to the interval between 0 and 1. When we add predictor variables, we do so on the cumulative logit scale. The link function takes care of converting the parameter estimates to the proper probability scale.

We first compute the cumulative probabilities from the histogram. The discrete proportion of each EF ranking.
```{r}
pr_k <- as.vector(table(df2$maxEF) / nrow(df2))
pr_k_all <- as.vector(table(All_Tornadoes$mag) / nrow(All_Tornadoes))
cum_pr_k <- cumsum(pr_k)
cum_pr_k_all <- cumsum(pr_k_all)

plot.df <- data.frame(maxEF = 0:5, pr_k, cum_pr_k, cum_pr_k_all)

(p1 <- ggplot(plot.df, aes(x = maxEF, y = cum_pr_k_all)) +
  geom_point(color = "gray70") +
  geom_line(color = "gray70") +
  geom_point(aes(y = cum_pr_k)) +
  geom_line(aes(y = cum_pr_k)) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous(minor_breaks = 0:5) +
    xlab("Maximum EF Rating") +
    ylab("Cumulative Proportion") +
    ggtitle("A") +
  theme_minimal()
)
```

Then to re-describe the histogram as log-cumulative odds, we need a series of intercept parameters. Each intercept will be on the log-cumulative-odds scale and stand in for the cumulative probability of each rating.
$$
\log \frac{\Pr(y_i \le k)}{1 - \Pr(y_i \le k)} = \alpha_k
$$
where $\alpha_k$ is an 'intercept' unique to each possible EF rating $k$. 

We compute these intercept parameters directly.
```{r}
( lco <- rethinking::logit(cum_pr_k) )
( lco_all <- rethinking::logit(cum_pr_k_all) )

plot.df$lco <- lco
plot.df$lco_all <- lco_all

(p2 <- ggplot(plot.df[1:5, ], aes(x = maxEF, y = lco_all)) +
  geom_point(color = "gray70") +
  geom_line(color = "gray70") +
  geom_point(aes(y = lco)) +
  geom_line(aes(y = lco)) +
  scale_y_continuous() +
  scale_x_continuous(minor_breaks = 0:4) +
  xlab("Maximum EF Rating") +
  ylab("Log-Cumulative-Odds") +
    ggtitle("B") +
  theme_minimal()
)
```
### Figure: Log-cumulative odds of a tornado by maximum EF rating for all tornadoes (gray) and for tornadoes occurring in large clusters (black)**

Note that the cumulative logit for the highest EF rating is infinity. This is because log(1/(1 - 1)) = $\infty$. This is always the case so we do not need a parameter for it. We get it for free from the law of total probability. So for $K$ = 6 possible maximum EF ratings we only need $K$ - 1 = 5 intercepts.

Combine into single figure.
```{r}
library(ggpubr)
ggarrange(p1, p2)
```

What we really want is the prior distribution of these intercepts. This allows us to take into account sample size and prior information, as well as insert predictor variables.

To use Bayes' theorem to compute the posterior distribution of these intercepts, we will need to compute the likelihood for each possible EF rating. So the last step in constructing the basic model fitting engine for these ordered categorical outcomes is to use cumulative probabilities $\Pr(y_i \le k)$ to compute the likelihood $\Pr(y_i = k)$.

```{r}
plot.df$ys <-  plot.df$cum_pr_k - plot.df$pr_k

ggplot(plot.df, aes(x = maxEF, y = cum_pr_k)) +
  geom_segment(aes(x = maxEF, xend = maxEF, y = ys, yend = cum_pr_k), size = 1.3, color = "gray70") +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1)) +
    xlab("Maximum EF Rating") +
    ylab("Cumulative Proportion") +
  theme_minimal()
```

Cumulative probability and ordered likelihood. The horizontal axis displays possible observable damage ratings, from 0 through 5. The vertical axis displays cumulative probability. The points show cumulative probability. These keep getting higher with each successive EF rating. The gray line segments show the discrete probability of each EF rating. These are the likelihoods that go into Bayes’ theorem.

In code form
$$
\begin{aligned} 
\hbox{R}_i &\sim \hbox{Ordered}(\mathbf{p}) \\ 
\hbox{logit}(p_k) &= \alpha_k \\
\alpha_k &\sim \hbox{Normal}(0, 10)
\end{aligned}
$$

The Ordered distribution is a categorical distribution that takes a vector $p = \{p_0, p_1, p_2, p_3, p_4\}$ of probabilities for each EF rating below the highest (EF5). Each response value $k$ in this vector is defined by its link to an intercept parameter ($\alpha_k$). 

### Adding a predictor variable

To include predictor variables, we define the log-cumulative-odds of each EF rating $k$ as a sum of its intercept $\alpha_k$ and a typical linear model. Suppose for  example we want to add a predictor $x$ to the model. We do this by defining a linear model $\phi_i = \beta x_i$. Then each cumulative logit becomes
$$
\begin{aligned}
\log \frac{\Pr(y_i \le k)}{1 - \Pr(y_i \le k)} &= \alpha_k - \phi_i \\
\phi_i &= \beta x_i
\end{aligned}
$$

The form ensures the correct ordering of the EF ratings while allowing for changes in the likelihood of each individual value as the predictor $x_i$ changes value. As the log-cumulative odds of every EF value ($k$) below the maximum decreases, the probability mass shifts upwards toward higher EF ratings. This is analogous to what happens in quantile regression (see \cite{JaggerElsner2009}).

\[
\phi_i = \beta_{Year}Year_i + \beta_D D_i + \beta_{CAPE}CAPE_i + \beta_{HLCY}HLCY_i + \beta_{BS} BS_i + \beta_{CIN}CIN_i + \beta_M M_i + \beta_G G_i
\]

where Year_i indicates the year of tornado i and D_i indicates the distance of tornado i to the nearest town/city. CAPE_i, HLCY_i, BS_i, and CIN_i indicate the values of the corresponding environmental variables from the group assigned to tornado i. All tornadoes within the group are given the same value. Month (M_i) and group (G_i) are random offset effects so the coefficients (\beta_M and \beta_G) are vectors of length 12 and 212, respectively. 

Create scaled predictor variables. Remove ID = 200305302651. maxHLCY is way too large.
```{r}
df2 <- df2[df2$ID != 200305302651, ]

df2$Yrs <- scale(df2$Year)
df2$dists <- scale(df2$dist)
df2$CAPEs <- scale(df2$maxCAPE)
df2$HLCYs <- scale(df2$maxHLCY)
df2$BSs <- scale(df2$maxBS)
df2$CINs <- scale(df2$minCIN)
```

### Mixed effects model

Group number indexed from 1 to number of groups.
```{r}
nT <- as.vector(table(df2$ID))
id <- rep(1:length(nT), times = nT)
df2$id <- id
df2$maxEF1 <- df2$maxEF + 1 # can not use 0
```

Use the **brms** package.

Start by setting the family and the model formula. Get priors. 
```{r}
library(brms)
family <- brms::cumulative(threshold = "flexible")
formula <- maxEF1 ~ 1

get_prior(formula, data = df2, family = family)

prior <- brm(formula = formula,
           data = df2,
           family = family,
           prior = set_prior("student_t(7, 0, 10)", class = "Intercept"),
           sample_prior = "only",
           seed = 9121)
prior_out <- predict(prior, probs = c(0, 1))
head(prior_out)

fit0 <- brm(formula = formula,
           data = df2,
           family = family,
           prior = set_prior("student_t(7, 0, 10)", class = "Intercept"),
           seed = 9121)
fixef(fit0)

fit0_out <- predict(fit0, probs = c(0, 1))
head(fit0_out)
```

Since there are a lot of tornadoes, the posterior for each intercept is quite precisely estimated, as we can see from the small standard deviations. To get cumulative probabilities back:
```{r}
logistic(fixef(fit0))
```

These are the same (nearly) as the values in cum_pr_k that we computed above. But now we also have a posterior distribution around these values, and we’re ready to add predictor variables to the model.

Model with predictors. Start with some default priors (for example "normal(0,5)" for the class = "b"). Sample from the priors and check the predictive distribution of the response. Adjust the priors accordingly. https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations
```{r}
family <- brms::cumulative(threshold = "flexible")
formula <- maxEF1 ~ Yrs + dists + CAPEs + HLCYs + BSs + CINs  + (1|mo) + (1|id)

get_prior(formula, data = df2, family = family)

prior2 <- brm(formula = formula,
           data = df2,
           family = family,
           prior = c(set_prior("normal(0, 1)", class = "b"),
                     set_prior("student_t(3, 0, 10)", class = "Intercept"),
                     set_prior("student_t(3, 0, 10)", class = "sd")),
           sample_prior = "only",
           seed = 9121)

prior_out2 <- predict(prior2, probs = c(0, 1))
```

With normal(0, 5) the probabilities are highest for the first and sixth categories. This u-shaped distribution is diminished by using normal(0, 1). Making the standard deviation smaller introduces divergent transitions.

Fit the model.
```{r}
fit1 <- brm(formula = formula,
            data = df2,
            family = family,
            prior = c(set_prior("normal(0, 1)", class = "b"),
                     set_prior("student_t(3, 0, 10)", class = "Intercept"),
                     set_prior("student_t(3, 0, 10)", class = "sd")),
           control = list(max_treedepth = 15),
           seed = 9121)

fixef(fit1)

#save(fit1, file = "fit.RData")
fit1_out <- predict(fit1, probs = c(0, 1))
# stancode(fit1)

fit1.df <- posterior_samples(fit1)
```

Examine potential interaction between BS and CAPE.
```{r}
formula2 <- maxEF1 ~ Yrs + dists + BSs * CAPEs + HLCYs + CINs  + (1|mo) + (1|id)

fit2 <- brm(formula = formula2,
            data = df2,
            family = family,
            prior = c(set_prior("normal(0, 1)", class = "b"),
                     set_prior("student_t(3, 0, 10)", class = "Intercept"),
                     set_prior("student_t(3, 0, 10)", class = "sd")),
           seed = 9521)

fixef(fit2)
#save(fit2, file = "fit2.RData")

library(bayestestR)
equivalence_test(fit2)
```
There is no evidence to support an interaction between BS and CAPE.

Here we examine whether the effect of bulk shear is different across different EF ratings.
```{r, eval=FALSE}
formula3 <- maxEF1 ~ Yrs + dists + CAPEs + HLCYs + cs(BSs) + CINs + (1|mo) + (1|id)
family3 <- "sratio"
fit3 <- brm(formula = formula3,
            data = df2,
            family = family3,
            prior = c(set_prior("normal(0, 1)", class = "b"),
                    set_prior("student_t(3, 0, 10)", class = "Intercept"),
                    set_prior("student_t(3, 0, 10)", class = "sd"),
                    set_prior("normal(-1, 2)", coef = "BSs")),
             seed = 9112)
fixef(fit3)
#save(fit3, file = "fit3.RData")
```
Apparently not. The uncertainty bounds on the estimates overlap across the EF ratings. 

Here we examine whether the trend is different across different EF ratings.
```{r, eval=FALSE}
formula4 <- maxEF1 ~ cs(Yrs) + dists + CAPEs + HLCYs + BSs + CINs + (1|mo) + (1|id)
family4 <- "sratio"
fit4 <- brm(formula = formula4,
            data = df2,
            family = family4,
            prior = c(set_prior("normal(0, 1)", class = "b"),
                    set_prior("student_t(3, 0, 10)", class = "Intercept"),
                    set_prior("student_t(3, 0, 10)", class = "sd"),
                    set_prior("normal(-1, 2)", coef = "BSs")),
             seed = 9112)
fixef(fit4)
#save(fit4, file = "fit4.RData")
```
Here we see an increase in EF0 relative to EF1s and EF2s.

Here we examine whether the effect of distance to city is different across different EF ratings.
```{r, eval=FALSE}
formula5 <- maxEF1 ~ dists + cs(CAPEs) + HLCYs + BSs + (1|mo) + (1|id)
family5 <- "sratio"
fit5 <- brm(formula = formula5,
            data = df2,
            family = family5,
            prior = c(set_prior("normal(0, 1)", class = "b"),
                    set_prior("student_t(3, 0, 10)", class = "Intercept"),
                    set_prior("student_t(3, 0, 10)", class = "sd"),
                    set_prior("normal(-1, 2)", coef = "BSs")),
                    control = list(max_treedepth = 15, adapt_delta = .9),
             seed = 9112)
fixef(fit5)
save(fit5, file = "fit5.RData")
fit5.df <- posterior_samples(fit5)
```

```{r, eval=FALSE}
formula6 <- maxEF1 ~ dists + CAPEs + HLCYs + BSs + (1|mo) + (1|id)
family6 <- brms::cumulative(threshold = "flexible")
fit6 <- brm(formula = formula6,
            data = df2,
            family = family6,
            prior = c(set_prior("normal(0, 1)", class = "b"),
                    set_prior("student_t(3, 0, 10)", class = "Intercept"),
                    set_prior("student_t(3, 0, 10)", class = "sd"),
                    set_prior("normal(-1, 2)", coef = "BSs")),
             seed = 9112)
fixef(fit6)
save(fit6, file = "fit5.RData")
fit6.df <- posterior_samples(fit6)
```


Cross validation. This takes a very long time.
```{r, eval=FALSE}
kfold1 <- kfold(fit, folds = "stratified", group = "id")
kfold <- kfold(fit, group = "id", chains = 1) 
```

### Hypothesis test
```{r}
summary(fit1)
```

Test the hypothesis that HLCYs is more important that CAPE.
```{r}
(hyp <- hypothesis(fit6, "HLCYs > CAPEs", class = "b"))
plot(hyp)

hypothesis(fit6, "dists < 0", class = "b")
```

#### Bulk shear

Consider how the model handles bulk shear. Set all variables to zero except distance to city and bulk shear. Set the distance to city to a small value (near the city). The scale and center parameters of the bulk shear variable are obtained and then extremes values are used.
```{r}
BSs <- scale(df2$maxBS)
scale <- attr(BSs, "scaled:scale")
center <- attr(BSs, "scaled:center")
( BSlo <- (15 - center) / scale )
( BShi <- (45 - center) / scale )
```

Line plots.
```{r}
dists <- -1.3
CAPEs <- 0
HLCYs <- 0
BSs <- c(BSlo, BShi)

tpk <- numeric()
for ( s in 1:10 ) {
    p <- fit6.df[s, ]
    ak <- as.numeric(p[1:5])
    phi <- p$b_dists * dists + p$b_BSs * BSs
    pk <- rethinking::pordlogit( 1:5 , a = ak , phi = phi )
    tpk <- rbind(tpk, pk)
}

tpk.df <- as.data.frame(tpk)
tpk.df$X <- rep(c(15, 45), times = 10)
tpk.df$S <- rep(1:10, each = 2)
names(tpk.df) <- c("EF0", "EF1", "EF2", "EF3", "EF4", "X", "S")

( p3 <- ggplot(tpk.df, aes(x = X, y = EF0, group = S)) +
  geom_line(color = "#EFF3FF") +
  geom_line(aes(y = EF1), color = "#BDD7E7") +
  geom_line(aes(y = EF2), color = "#6BAED6") +
  geom_line(aes(y = EF3), color = "#3182BD") +
  geom_line(aes(y = EF4), color = "#08519C") +
  scale_y_continuous(limits = c(0, 1)) +
  scale_x_continuous() +
  xlab("Bulk Shear (m/s)") + ylab("Cumulative Proportion") +
  ggtitle(label = "A") +
  theme_minimal() )
```

Area plots.
```{r}
#p <- fit.df[1, ]
p <- colMeans(fit6.df)

ak <- as.numeric(p[1:5])
phi <- p[6] * dists + p[9] * BSs[1]
dk1 <- rethinking::dordlogit( 1:5 , a = ak , phi = phi )

phi <- p[6] * dists + p[9] * BSs[2]
dk2 <- rethinking::dordlogit( 1:5 , a = ak , phi = phi )

dk <- c(dk1, dk2)

tpk.df <- data.frame(dk,
                     BS = rep(c(15, 45), each = 5),
                     EF = factor(rep(c("0", "1", "2", "3", "4"), times = 2), 
                                 levels = c("4", "3", "2", "1", "0")))

( p4 <- ggplot(tpk.df, aes(x = BS, y = dk)) + 
  geom_area(aes(fill = EF), color = "white") +
  scale_fill_brewer(type = "seq", direction = -1) +
  xlab("Bulk Shear (m/s)") + ylab("Cumulative Proportion") +
  theme_minimal() +
  ggtitle(label = "B") 
    )

ggarrange(p3, p4, nrow = 1)
```
### Figure: Predicted proportion of tornadoes by EF category. A. Predictions from the first 10 posterior samples. B. Predictions based on the average over 4000 posterior samples.

Another way to examine the model results is to display the marginal effects. Here we examine the marginal effect of bulk shear. We use the mean value for all the other covariates.

Marginal plot. Bulk shear.
```{r}
gg <- marginal_effects(fit6, categorical = TRUE)
ggBS <- gg[[4]]
levels(ggBS$cats__) <- c("0", "1", "2", "3", "4", "5")

BSs <- scale(df2$maxBS)
scale <- attr(BSs, "scaled:scale")
center <- attr(BSs, "scaled:center")

ggBS <- ggBS %>%
  mutate(BS = BSs * scale + center)

( p5 <- ggplot(ggBS, aes(x = BS, y = estimate__, col = cats__)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = cats__), 
              alpha = .7, col = NA) +
  scale_color_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  scale_fill_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  scale_y_continuous(limits = c(0, 1)) +
  ylab("Relative Chance of a Tornado\nCausing EF-level Damage") +
  xlab("Bulk Shear (m/s)") +
    ggtitle("A") +
  theme_minimal() )
```

Marginal plot. Distance to nearest city.
```{r}
ggDist <- gg[[1]]
levels(ggDist$cats__) <- c("0", "1", "2", "3", "4", "5")

dists <- scale(df2$dist)
scale <- attr(dists, "scaled:scale")
center <- attr(dists, "scaled:center")

ggDist <- ggDist %>%
  mutate(dist = dists * scale + center)

( p6 <- ggplot(ggDist, aes(x = dist/1000, y = estimate__, col = cats__)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = cats__), 
              alpha = .7, col = NA) +
  scale_color_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  scale_fill_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  scale_y_continuous(limits = c(0, 1)) +
  ylab("Relative Chance of a Tornado\nCausing EF-level Damage") +
  xlab("Distance to nearest city (km)") +
    ggtitle("C") +
  theme_minimal() )
```

Marginal plot. CAPE.
```{r}
ggCAPE <- gg[[2]]
levels(ggCAPE$cats__) <- c("0", "1", "2", "3", "4", "5")

CAPEs <- scale(df2$maxCAPE)
scale <- attr(CAPEs, "scaled:scale")
center <- attr(CAPEs, "scaled:center")

ggCAPE <- ggCAPE %>%
  mutate(CAPE = CAPEs * scale + center)

( p7 <- ggplot(ggCAPE, aes(x = CAPE, y = estimate__, col = cats__)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = cats__), 
              alpha = .7, col = NA) +
  scale_color_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  scale_fill_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  scale_y_continuous(limits = c(0, 1)) +
  ylab("Relative Chance of a Tornado\nCausing EF-level Damage") +
  xlab("CAPE (J/kg)") +
    ggtitle("B") +
  theme_minimal() )
```

Marginal plot. Helicity. Units on helicity do not appear to be correct. Should be in the range of 50-400 m^2/s^2. Removed one group to fix this.
```{r}
ggHLCY <- gg[[3]]
levels(ggHLCY$cats__) <- c("0", "1", "2", "3", "4", "5")

HLCYs <- scale(df2$maxHLCY)
scale <- attr(HLCYs, "scaled:scale")
center <- attr(HLCYs, "scaled:center")

ggHLCY <- ggHLCY %>%
  mutate(HLCY = HLCYs * scale + center)

( p8 <- ggplot(ggHLCY, aes(x = HLCY, y = estimate__, col = cats__)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = cats__), 
              alpha = .7, col = NA) +
  scale_color_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  scale_fill_brewer(name = "EF", label = c("0", "1", "2", "3", "4", "5"), palette = "Blues") +
  scale_y_continuous(limits = c(0, 1)) +
  ylab("Relative Chance of a Tornado\nCausing EF-level Damage") +
  xlab("Storm Relative Helicity (J/kg)") +
    ggtitle("D") +
  theme_minimal() )
```

```{r}
ggarrange(p5, p7, p6, p8)
```
### Figure: Marginal effects of the covariates. (A) Bulk shear, CAPE (B), (C) distance to nearest town, and (D) storm relative helicity.

https://aosmith.rbind.io/2019/05/13/small-multiples-plot/
```{r}
library(egg)
ggarrange(p5 +
              theme(legend.position = 'none'), 
          p7 + 
               theme(axis.text.y = element_blank(),
                     axis.ticks.y = element_blank(),
                     axis.title.y = element_blank()),
          nrow = 1)

M6 <- ggarrange(p5 +
              theme(legend.position = 'none'), 
          p7 + 
               theme(axis.text.y = element_blank(),
                     axis.ticks.y = element_blank(),
                     axis.title.y = element_blank()),
          p6 +
              theme(legend.position = 'none'),
          p8 + 
              theme(axis.text.y = element_blank(),
                    axis.ticks.y = element_blank(),
                    axis.title.y = element_blank(),
                    legend.position = 'none'),
          nrow = 2)
```
          

Add some specific numbers. Make predictions with the predict() function specifying values.
```{r}
predict(fit1, newdata = data.frame(Yrs = 0, dists = 0, CAPEs = 0, HLCYs= 0, BSs = 0, CINs = 0, mo = 1:12, id = 5))
```

